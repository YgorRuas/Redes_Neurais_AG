# Experimentos de redes neurais artificiais

<p align="justify"> Nesse README podemos encontrar a resolu√ß√£o de problemas de Redes Neurais Artificiais. Para tal, a linguagem Jupyter Notebook √© predominante nos c√≥digos, mas tamb√©m, √© usada a linguagem de python principalmente em funcoes.py. Ademais, foi usado o Black para uma melhor formata√ß√£o dos c√≥digos. </p>

## O que s√£o Rdese Neurais?

<p align="justify">  Redes neurais s√£o algoritmos que se baseiam em um processamento de dados similar ao processamento de informa√ß√µes de um c√©rebro humano que ocorrem por meio de um conjunto extremamente complexo de c√©lulas, os neur√¥nios, estes que s√£o respons√°veis pela tomada de decis√£o do racioc√≠nio e corpo humanos. Portanto, os neur√¥nios s√£o formados por dendritos, conjuntos de terminais de entrada, pelo corpo central, e pelos ax√¥nios, al√©m dos terminais de sa√≠das. </p>
<p align="justify">  Desse modo, a comunica√ß√£o dos neur√¥nios ocorrem por meia da sinapse, regi√£o onde dois neur√¥nios entram em contato e transmitem impulsos nervosos. Estabelecido o funcionamento biol√≥gico, assim, observa-se que o comportamento das redes neurais √© similar ao de um neur√¥nio visto que a estrutura computacional √© baseada em um sistema tr√™s camdas, sendo um de input, dado pelo valor de entrada; a camada escondida e um neur√¥nio que soma os valores de entrada recebidos e incrementa um vi√©s; e por fim, o output, este sendo um valor de sa√≠da das opera√ß√µes. </p>

<center>
<img src='./Rededogcat.gif' style="width:900px;height:400px"/>
</center>

### Backpropagation

√â um algoritmo muito utilizado em redes neurais devido o ajuste do peso das conex√µes entre os neur√¥nios com o objetivo de minimizar o erro entre as sa√≠das da rede e os valores desejados. Assim, o processo √© dividido em duas etapas: propaga√ß√£o para frente `forward propagation` e retropropaga√ß√£o do erro `backwardpropagation`. 
  
<p align="justify">  Desse modo, o primeiro processo considerando os dados de entrada alimentados na rede e os valores de sa√≠da c√°lculados a partir da ativa√ß√£o dos neur√¥nios durante a etapa de propaga√ß√£o para frente, √© que os dados de entrada s√£o alimentados na rede, j√° os de sa√≠das s√£o calculadas atrav√©s das ativa√ß√µes dos neur√¥nios, por sua vez, o segundo processo, determina que o erro entre as sa√≠das da rede e os valores desejados, assim √© propagado de volta para a rede. Ademais, os pesos das conex√µes s√£o atualizados de forma a se obter um erro m√≠nimo, utilizando o gradiente que √© calculado atrav√©s da derivada parcial do erro em rela√ß√µes aos pesos e s√£o repetidos at√© que a rede forne√ßa uma sa√≠da com um erro aceit√°vel.  </p>

<center>
<img src='./Backprogation_Example.gif' style="width:700px;height:400px"/>
</center>

## Arquivos

<b> Arquivos .py</b>:

<p style='text-align: justify'> üìÇ <a href="https://github.com/YgorRuas/Redes_Neuro_Anais/blob/main/RedesNeurais/classes.py">classes.py</a> - Neste arquivo est√£o alocados todas as classes criadas com o objetivo de auxiliar na otimiza√ß√£o do c√≥digo..</p>

<p style='text-align: justify'> üìÇ <a href="https://github.com/YgorRuas/Redes_Neuro_Anais/blob/main/RedesNeurais/constantes.py">constantes.py</a> - Neste arquivo est√£o alocadas todas as constantes utilizadas nos c√≥digos fonte para a resolu√ß√£o dos exerc√≠cios. √â um ambiente destinado para guardar e armazenar as constantes de modo a otimizar o c√≥digo.</p>

<p style='text-align: justify'> üìÅ <a href="https://github.com/YgorRuas/Redes_Neuro_Anais/blob/main/RedesNeurais/funcoes.py">funcoes.py</a> - Neste arquivo, ser√£o armazenadas todas as fun√ß√µes sendo utilizadas ao longo do tempo nos experimentos/projetos. Com este arquivo, a reutiliza√ß√£o de fun√ß√µes nos mais diversos arquivos da pasta ser√° possibilitada, assim como a organiza√ß√£o ser√° evidenciada.</p>

<p style='text-align: justify'> üìÅ <a href="https://github.com/YgorRuas/Redes_Neuro_Anais/blob/main/RedesNeurais/README.md">README.md</a> - √â o arquivo do readme, do reposit√≥rio referente a Redes Neurais, onde fica o cart√£o de visita para essa se√ß√£o do reposit√≥rio.</p>


## Descri√ß√£o dos experimentos

### R.01 - Derivadas

Esse material n√£o possu√≠ um desafio, mas sim, uma introdu√ß√£o a derivadas, sendo de extrema impot√¢ncia no decorrer do reposit√≥rio.

### R.02 - Classes

A respeito desse experimento, tem-se que ele foca principalmente na introdu√ß√£o do funcionamento das classes. Desse modo, entendendo a estrutura, como s√£o os m√©todos `dunder` e n√£o `dunder`, fomos introduzidos a como alterar a classe e tamb√©m como realizar altera√ß√µes fora dela. Assim, por meio dos novos conceitos, um desafio foi proposto, cujo objetivo √© adicionar um novo argumento no m√©todo `__init__`.

### R.03 - Construindo um Grafo Automaticamente

Por meio desse experimento, fomos introduzidos a como iniciar a constru√ß√£o de uma rede neural artificial. Assim entendendo a estrutura, como funcionam os operadores de adi√ß√£o e multiplica√ß√£o, a estrutura dos progenitores, o operador m√£e e como plotar um grafo. Desse modo, compreendendo como funcionam as classe, foi poss√≠vel criar uma que gerasse automaticamente um `grafo computacional`, este que representa todas as opera√ß√µes matem√°ticas que ocorreram ao se computar um certo valor, onde o valor foi com base no grafo feito na aula do dia 27/04/2023. Portanto, o valor utilizado foi $g$, para que fosse poss√≠vel visualizar todo o grafo, assim, sendo √∫til para experimentos futuros os quais necessitam do grafo computacional como na computa√ß√£o dos gradientes  para realizar o `backpropagation`.

Imagem gerada pelo Graphviz Online, para plotar a rede neural criada no experimento R.03

![image](https://user-images.githubusercontent.com/106711102/236648255-cf1bd3c1-328a-4ae4-8a2d-22d59af40e8f.png)

### R.04 - Computando Gradientes Locais

O presente desafio, foca na implementa√ß√£o de gradientes locais e do c√°lculo de backpropagation para a classe usada no experimento anterior, onde, buscamos uma otimiza√ß√£o e automatiza√ß√£o do processo, portanto, usamos m√©todos como o `propagar` e algoritmos de `autograd`, resultando em uma classe capaz de criar um grafo a e executar o `backpropagation`, sem apresentar problemas matem√°ticos ou erro em algum dos v√©rtices, para a rede representada pelo grafo.

### R.05 - Finalizando a Classe Valor

Esse experimento, por meio das classes, apresenta algumas opera√ß√µes necess√°rias para que seja usada em redes neurais artificiais. Assim √© introduzido a classe `Valor`  a qual √© uma implementa√ß√£o que representa valores num√©ricos e permite opera√ß√µes como adi√ß√£o, multiplica√ß√£o, exponencia√ß√£o, divis√£o e as mesmas em ordem invertida, al√©m de conseguir a aplica√ß√£o da fun√ß√£o sigmoide. Assim, s√£o importantes pois muitas opera√ß√µes matem√°ticas s√£o realizadas nessas redes, como soma ponderada, multiplica√ß√£o por pesos, c√°lculos de ativa√ß√£o e propaga√ß√£o de gradientes. Portanto, ao implementar a classe `Valor` com essas funcionalidades, torna-se poss√≠vel utiliz√°-la como base para construir uma rede neural artificial mais completa.

### R.06 - Redes Neurais Artificiais

O notebbok referente a **Redes Neurais Artificiais**, nos apresenta a como criar uma rede neural artificial usando Python. Dessa maneira, construindo uma rede neural multicamadas de forma que seja poss√≠vel criar uma rede que seja capaz de processar informa√ß√µes e realizar opera√ß√µes com base em sinapses artificiais ponderadas, somas, propaga√ß√£o dos dados entre outros que foi determinado na classe `Valor` e criado no **experimento R.04**, mas tamb√©m dispon√≠vel no arquivo `classes.py`. Al√©m disso, √© introduzida a ideia de uma camada de neur√¥nios e tamb√©m √© apresentado o conceito de Multilayer Perceptron (MLP).

### R.07 - Treinando uma rede neural

A respeito desse desafio o principal objetivo desse notebook √© treinar uma rede neural artificial tipo Multilayer Perceptron usando Python. Assim, √© apresentado um exemplo de treinamento de uma rede neural artificial tipo Multilayer Perceptron (MLP) usando Python puro, este que descreve a cria√ß√£o das classes necess√°rias para construir a rede neural, incluindo a classe Neuronio, a classe Camada e a classe MLP, al√©m de importar a classe qeu foi crida e usada nos materias anteriores `Valor`. Al√©m disso foi intoduzido a fun√ß√£o de perda `loss function`, esta que calcula a soma dos erros quadr√°ticos entre as previs√µes e os valores verdadeiros.

### R.08 - Treinando uma rede neural usando pytorch

Em sequ√™mcia ao notebook anterior (**experimento R.07**), esse poss√∫i o objetivo de treinar uma rede neural artificial tipo Multilayer Perceptron usando pytorch. Assim, √© ressaltado os motivos para utilizar PyTorch, como sua otimiza√ß√£o eficiente, ampla variedade de funcionalidades prontas para uso, suporte ao treinamento em GPUs e sua relev√¢ncia tanto na academia quanto no mundo corporativo, sendo bem mais eficiente do que a rede neural que foi criada no decorrer do semestre. Portanto, atrav√©s do exemplo apresentado (dos diamantes), √© poss√≠vel compreender as etapas envolvidas no treinamento de uma rede neural, desde a prepara√ß√£o dos dados at√© a avalia√ß√£o do modelo, que foi feito um treinamento e depois o teste, evidenciando a eficacia desse m√©todo e da biblioteca pytorch.

## Contribui√ß√µes

O presente git possu√≠ como √∫nico autor [Ygor Ruas](https://github.com/YgorRuas/Redes_Neuro_Anais/tree/main), contudo, o arquivo advem da disciplina de Redes Neurais e Algoritmos Geneticos do Professor [Cassar. Daniel](https://github.com/drcassar/template_rnag), onde foI realizado uma ramifica√ß√£o para o desenvolvimento desse git. Al√©m disso, como contribui√ß√µes temos os alunos da primeira turma da ilum, estes que ajudaram de forma coletiva em parte da montagem e estrutura√ß√£o dos c√≥digos.

## Agradecimentos

<p align="justify"> Agradecimento especial ao Professor Daniel Cassar pelo apoio no decorrer da disciplina e pelas aulas ministradas. </p>
